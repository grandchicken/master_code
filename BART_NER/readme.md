### Warmup
#### 是什么
Warmup是针对学习率优化的一种方式，Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches,再修改为预先设置的学习率来进行训练。

#### 为什么
1. 在实际中，由于训练刚开始时，训练数据计算出的梯度 grad 可能与期望方向相反，所以此时采用较小的学习率 learning rate，随着迭代次数增加，学习率 lr 线性增大，增长率为 1/warmup_steps；迭代次数等于 warmup_steps 时，学习率为初始设定的学习率；
2. 另一种原因是由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。
3. 迭代次数超过warmup_steps时，学习率逐步衰减，衰减率为1/(total-warmup_steps)，再进行微调。
4. 刚开始训练时，学习率以 0.01 ~ 0.001 为宜， 接近训练结束的时候，学习速率的衰减应该在100倍以上

